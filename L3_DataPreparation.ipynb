{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089363e9-1bf8-4ace-8487-0670192ad039",
   "metadata": {},
   "source": [
    "# Lesson 3: Data Preparation\n",
    "<b><u>Instructors</b></u>: Ishan Mishra, Phil Culliton\n",
    "\n",
    "\n",
    "<br>\n",
    "Data preparation is particularly important when doing Remote Data Science and Machine Learning. This is because the data scientist doesn't inherently have the ability to freely check how clean the data is, and whether it needs pre-processing. As such, the responsibility falls onto the Data Owner to ensure the data is clean, annotated, and usable.\n",
    "\n",
    "\n",
    "In this notebook, you'll walk in the steps of a Data Owner (someone who has new and potentially sensitive information) and learn about:\n",
    "<ul>\n",
    "    <li> Data Acquisition </li>\n",
    "    <li> Quality Checks </li>\n",
    "    <li> Annotation </li>\n",
    "    <li> Converting your data to a PyGrid compatible format </li>\n",
    "    <li> How to load data into the node </li>\n",
    "    <li> Linking data from multiple sources </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5cd9b-1ffa-4f9e-b585-3493d897bc60",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3eebeb-6139-4fcf-b7f0-cf2bd5c0d4b4",
   "metadata": {},
   "source": [
    "### Here's what you'll need to install for this lesson\n",
    "\n",
    "In this lesson, we'll be using the new library `matplotlib`. If you haven't already, please install it using:\n",
    "\n",
    "`conda install matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a2141-63fa-4248-a0ec-9323e779d6fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Data Acquisition!\n",
    "\n",
    "Data Acquisition focuses on generating and capturing data into a system. Broadly speaking, it's made up of two phases: <b> data harvesting</b>, and <b>data ingestion</b>. The former, we'll cover in this lesson, and the latter, we'll show you over the next two lessons.\n",
    "\n",
    "### 3.1.1 Data Harvesting\n",
    "\n",
    "In this course, we're using generated or pre-gathered datasets, but in the real world data comes in many forms and contexts - we'll cover some in the next section. How do you get your own data? The phrase \"data harvesting\" might sound like a callback to movies about robotic uprisings, but the wording is truly operative! In this process we are:\n",
    "\n",
    "1. Identifying sources of potentially fruitful data\n",
    "2. Building a process to gather it\n",
    "3. Running that process\n",
    "4. Carefully checking the outcomes\n",
    "5. Putting the data in a shareable format\n",
    "6. Delivering the data to intended recipient\n",
    "\n",
    "Thinking of the data gathering process as akin to a farmer's harvest is actually a useful framing of the relevant tasks and problems.\n",
    "\n",
    "- There may be better sources of data\n",
    "- The data may be flawed or need care and pruning\n",
    "- The data and format should align with the intended recipient's needs\n",
    "- etc.\n",
    "\n",
    "The key problem here, however, is `Is this the data that we need?` No amount of harvesting improvements or fancy formatting can solve for data that doesn't suit the problem you're trying to solve in some key aspect. To help with this, let's consider some useful ways to think about data.\n",
    "\n",
    "### 3.1.2 The 4 V's of Data \n",
    "\n",
    "When it comes to thinking about data, there are usually four major things to think about, commonly referred to in the industry as the 4 V's of data. They are:\n",
    "\n",
    "<ol>\n",
    "<li><b> Volume </b> refers to the quantity or amount of data in question.\n",
    "An example of low volume: if you're collecting sensitive data about people with a rare condition.\n",
    "An example of high volume would be most social media applications that you've heard of. For instance, Facebook has more users than China has people. And each of those people are making posts, uploading pictures, liking content- that adds up to trillions of photos that they can use for data science and machine learning. </li>\n",
    "<br>\n",
    "<li><b> Velocity </b> refers to the rate at which new data is being gathered or collected.\n",
    "For instance, if you're a company, performance reviews might only come once a quarter.\n",
    "But if you're YouTube, then in one day, you have over 700,000 hours of new videos added. For context, that's longer than the average human lifespan. So if a new person was born tomorrow, and they spent every moment of their life just trying to watch the YouTube videos uploaded on the day they were born, on average they wouldn't be able to get through them all. </li>\n",
    "<br>\n",
    "<li><b> Variety </b> refers to the diversity of the data that's being collected. For example, think of the difference between a dataset consisting of polls, and a dataset consisting of emails. No two emails are necessarily quite the same. They could contain quite literally, anything- text about anything, pictures of anything, attachments of any kind, etc. </li>\n",
    "<br>\n",
    "\n",
    "<li><b> Value </b> refers to the idea that not all kinds of data are of equal value. Let's say you're collecting medical images, and some of the images were corrupted during the acquisition process, and were blurry and grainy as a result. That data isn't quite as valuable as a pristine scan.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d908198-3f7f-4cb3-bea3-587026333b8e",
   "metadata": {},
   "source": [
    "### 3.1.3 Hands-on\n",
    "In our case, let's say we use data concerning the number of COVID cases per country. Lets load it and take a look at the first few rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from os import path as p\n",
    "\n",
    "BASE_FOLDER = Path(p.abspath(p.curdir))\n",
    "DATA_FOLDER = BASE_FOLDER / \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a85288-4f43-4732-98b1-1b9e3bc6eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "if DATA_FOLDER.exists():\n",
    "    datafile_ref = DATA_FOLDER / \"L3_raw_data.csv\"\n",
    "else:\n",
    "    datafile_ref = \"https://raw.githubusercontent.com/OpenMined/PySyft/dev/notebooks/course3/dataset/L3_data.csv\"\n",
    "\n",
    "raw_data = pd.read_csv(datafile_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5c8e2-642c-4686-921c-51b9ca9ab89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee93c43-ee4f-4848-ba08-4a31b7366805",
   "metadata": {},
   "source": [
    "In this dataset, each column corresponds to a <b> country</b>, each row corresponds to a new <b> month </b> where data was collected, and each value in this DataFrame corresponds to the number of COVID19 cases in the country at the start of that month. \n",
    "\n",
    "So for instance, Country 0 had `2280` COVID cases at the start of when this data was collected (`row 0`), and only `451` when the data was last collected (`row 53`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d63f95-0b88-428d-94a9-58bb588aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"0\"][0], raw_data[\"0\"][53]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3080f0-973d-4437-a133-21193e315bc3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4727484-454a-4a96-b9e1-a806997137a4",
   "metadata": {},
   "source": [
    "## 3.2 Quality Check\n",
    "Checking the quality of a dataset can involve finding missing values, identifying outliers and anomalies (using methods such as an [Isolation Forest](https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html) or [k-Nearest Neighbours](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)) or visualizing the dataset. \n",
    "It might also involve using external information that we know- for instance, about the sources or about how the data was collected.\n",
    "\n",
    "Let's walk through an example using the dataset again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001c907-e0c5-4a3c-b4ab-c7562e06c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913dbd4-dfa6-40e7-acf2-2285d401d79d",
   "metadata": {},
   "source": [
    "Now let's say, for instance, that when we were given the dataset, we were told that a lot of immigration and emmigration happened in the first month of this dataset, and as such a lot of double counting of COVID19 cases happened, and the global numbers of COVID19 were reported as twice of what they really were.\n",
    "\n",
    "We could try to naively tackle this by just dividing the data from the first month by half. Let's try it out and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5b683-61cc-4390-b8bb-ed6e0dfae50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data from the first month by half\n",
    "raw_data.iloc[0] /= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5b5ee-a7a6-40b1-a875-72e344e2cab2",
   "metadata": {},
   "source": [
    "Now let's try to visualize the dataset and see if anything suspicious looking appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf254c-7063-4d37-8b44-88964fa05d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the resultant data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db95d0-22a3-4a20-9593-f904bf7fcfca",
   "metadata": {},
   "source": [
    "Well that's odd- the data from the first month and second month appear to be exactly the same. If this happened to just one or two countries, that might be plausible, but for every country? That seems unlikely.\n",
    "\n",
    "There likely was an error when transferring this data, and a duplication error occurred. We can fix this by simply removing the first month from our dataset, and starting our analysis from the second month (row 1 in the table above), and onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b2214-8fe5-4787-8d56-5c5aa29acf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated row\n",
    "raw_data.drop([0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6741b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326c068-5e4a-40cf-ba7b-3e707469a1a3",
   "metadata": {},
   "source": [
    "At this point, we might decide to visualize our dataset and see if there are any obvious outliers or anomalies. Let's look at the first country!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7fef2-7ca3-440c-8bbd-6e17690f0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(raw_data.T.iloc[0], \"r-*\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0378da-80cd-43e4-bb0e-1778b132785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def plot_extrapolated_country(idx):\n",
    "    x = list(range(53))\n",
    "    y = raw_data.T.iloc[idx]\n",
    "    \n",
    "    plt.plot(y)\n",
    "    \n",
    "    for x1, y1 in zip(x, y):\n",
    "        plt.plot(x1, y1) #, 'ro')\n",
    "\n",
    "    z = np.polyfit(x, y, 2)\n",
    "    f = np.poly1d(z)\n",
    "\n",
    "    new_points = range(12)\n",
    "    for x2 in new_points:\n",
    "        plt.plot(55 + x2, f(55 + x2), 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252c345-c5e2-4f00-83c1-5f24f5992656",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_extrapolated_country(129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67efe9-764f-4ce0-9c71-a5519e949885",
   "metadata": {},
   "outputs": [],
   "source": [
    "thingies = []\n",
    "for i in range(raw_data.T.shape[0]):\n",
    "    x = range(53)\n",
    "    y = raw_data.T.iloc[i]\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    thingies.append(slope) #raw_data.T[i, -1] - raw_data.T[i, 0])\n",
    "    \n",
    "thingies.index(max(thingies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdaf6f8-d371-43fe-a064-f5a7a14073a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(raw_data).plot(legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d0da1-0e3e-4aaf-b26a-744e9a5c9dac",
   "metadata": {},
   "source": [
    "This seems alright, and doesn't seem to have any clear outliers or anomalies. \n",
    "\n",
    "We could do this for other countries, and maybe even employ other visualization methods such as clustering or PCA, depending on the type of data we're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70939805-bc5c-40d6-8fd1-183680c81db2",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584e48f-0ed6-4e0a-a0a6-465bf1383c91",
   "metadata": {},
   "source": [
    "## 3.3. Data Annotation\n",
    "\n",
    "In many cases, it's not just data that we need. It's annotated, or in other words, <b> labelled </b> data. For example, let's say I wanted to train a Neural Network to classify images of lungs based on whether or not the person had a healthy lung. Having the images alone wouldn't be enough; I'd also need a way of knowing which image corresponded to a healthy lung, and which ones don't.\n",
    "\n",
    "There are other kinds of annotation as well, such as semantic segmentation, bounding boxes, landmark and keypoint, and others. We won't go into too much detail about these, but feel free to check out some of the external resources and references at the bottom of this notebook!\n",
    "\n",
    "### 3.3.1 Common annotation types\n",
    "\n",
    "Most often annotations (or *labels*) will take the form of:\n",
    "1. Binary values (true or false)\n",
    "2. Classes (discrete states of being - cat or dog or airplane)\n",
    "3. Continuous values (population counts, dollars and cents, stock market returns)\n",
    "\n",
    "In the data we're using here, we've got a good example of number 3 - population counts. While it is bounded by physical realities, in theory it could be any number between -infinity and +infinity. A machine learning model would need to be able to predict anything in that range.\n",
    "\n",
    "Binary values and classes are considerably more constrained - while some multi-class problems have thousands (or millions!) of classes, they are all discrete and map to defined states.\n",
    "\n",
    "### 3.3.2 Creating annotations\n",
    "\n",
    "The process of labelling can be very simple, or very complex, depending on the nature of the annotations. For example, the annotations mentioned at the top of this section - healthy or unhealthy lungs - would most likely need to be:\n",
    "\n",
    "1. Diagnosed by a radiologist (or several radiologists)\n",
    "2. Recorded (and adjudicated when several radiologists were involved)\n",
    "3. Checked for accuracy\n",
    "\n",
    "More complex annotations (like identifying features of an unhealthy lung) would require bounding boxes or segmentations to be drawn as well, which would need to go through the same process.\n",
    "\n",
    "On the other hand, dog vs. cat labels might be applied very quickly, using an automated system or by having humans hand-label them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c468b9-885a-4979-96bc-2ea120784426",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261de034-408c-43e9-b55a-61ad66e55869",
   "metadata": {},
   "source": [
    "## 3.4. Data and PyGrid\n",
    "\n",
    "As mentioned earlier in the course, PyGrid is the library we've made that lets you deploy nodes and conduct remote data science. Currently, we primarily support data in the form of NumPy arrays; however, we plan to expand this further. Additionally, many common data science formats are easily convertible to NumPy arrays, such as Pandas DataFrames, Tensorflow or PyTorch tensors, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb8d90-2dac-4637-a938-485d1548f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a Pandas Dataframe to NumPy array\n",
    "print(f'raw_data is of type: {type(raw_data)}')\n",
    "raw_data = raw_data.values\n",
    "print(f'raw_data is of type: {type(raw_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69125e4-579e-4b1c-a2bf-d72040d58d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a PyTorch Tensor to a NumPy array\n",
    "import torch\n",
    "test_data = torch.Tensor([1, 2, 3, 4, 5])\n",
    "print(f'test_data is of type: {type(test_data)}')\n",
    "test_data = test_data.numpy()\n",
    "print(f'test_data is of type: {type(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06fe90-62bf-46bc-b99b-96710b497d5d",
   "metadata": {},
   "source": [
    "Additionally, to make use of Differential Privacy, the current version of PySyft (0.6.0) supports NumPy arrays having the np.int32 datatype. You can initialize a NumPy array to have this datatype as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1d0f3-e33b-4985-a9ac-0d0626a18792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "some_data = np.ndarray([1, 0, 1, 0, 1, 0], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab723c0a-d880-41ee-827c-8899b526d39d",
   "metadata": {},
   "source": [
    "You can also convert other NumPy arrays to the np.int32 datatype using the astype() method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c2b1a-8a1f-47c3-8de2-4fafab86a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.random((5, 5)) * 10\n",
    "print(f'random_data is of dtype: {random_data.dtype}')\n",
    "random_data = random_data.astype(np.int32)\n",
    "print(f'random_data is now of type: {random_data.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96026f42-62d8-4a55-8a7f-2a8ed830371c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568aaca-1d32-4ef7-8550-a16e4d48fcd0",
   "metadata": {},
   "source": [
    "## 3.5. Differential Privacy & Datasets\n",
    "\n",
    "### 3.5.1. A Primer on Differential Privacy\n",
    "\n",
    "You were introduced to the idea(s) behind Differential Privacy (DP) in Lesson 1. Feel free to go back to the video if you need a refresher!\n",
    "\n",
    "### 3.5.2. DP Metadata needed for PyGrid\n",
    "\n",
    "We've made it pretty simple to use PyGrid for Data Owners who want their data to be protected with DP; you only need to add 3 simple pieces of metadata:\n",
    "* min_val\n",
    "* max_val\n",
    "* entitities\n",
    "\n",
    "min_val and max_val correspond to the lowest and highest values found in the dataset. Ideally, they'd be data independent; so for instance, if you had regular .png images, the min_val could be 0 and the max_val could be 255, since a pixel value in that format can't go higher than that. If your dataset consisted of ages of various people, the min_val might be 0 years old, and the max_val might be 120 years old (the highest age any human being has ever reached).\n",
    "\n",
    "The tighter these bounds are, the more accurate and less privacy budget will be lost when a data scientist performs a computation with the data.\n",
    "\n",
    "On the other hand, entities corresponds to the Data Subjects; i.e. the people whose data is in the dataset, whose privacy we're protecting using Differential Privacy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 3.5.3 Understanding Why\n",
    "\n",
    "\n",
    "<b> Let's pause for a moment. </b> <br>\n",
    "Why do we have to specify min and max values? Why exactly is this necessary?  Let's try to understand with two contrasting examples:\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li>Let's say dataset A was a series of populations of various countries, such as [7,000,000, 10,000,000, 200,000, etc]. Because we're working with such large numbers (often in the millions and occassionally in the billions), it stands to reason that if we want to protect anonymity using differential privacy, we'd have to add a proportionate amount of noise. Adding a noise value of 5 to a result of 10,000,000 wouldn't really offer much in the way of privacy protection; we'd have to add noise values that's on the order of tens or maybe even hundreds of thousands to offer meaningful protection.</li>\n",
    "\n",
    "\n",
    "<li>Now let's say dataset B was a series of probabilities. For instance, let's say it was an array where every index corresponded to the probability that a person had a given disease. Something like: [0.0, 0.1, 0.9, 0.3, 0.4], which means the person has a 0% probability of having disease 1, a 0.1 probability (or 10% chance) of having disease 2, and so forth.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "If we're going to add noise to dataset A to obscure the results of querying it, we'd have to adjust the magnitude of noise being added proportionally. If we added a value of 10,000 to our probability values, the results would immediately become meaningless. In this case, we might see the magnitude of noise needed to be added to be on the order of 0.1 or 0.01. \n",
    "\n",
    "\n",
    "\n",
    "This is (one of) the reasons why annotating minimum and maximum values in differential privacy is so important- <b> it increases the accuracy of your computations, and still allows you to get meaningful results while still protecting privacy. </b>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 3.5.4 Annotation with PySyft\n",
    "\n",
    "Let's go back to our COVID dataset that we were preparing.\n",
    "\n",
    "Now, the actual process of annotating data with DP Metadata is pretty straight forward. We start by importing the PySyft library, and then by calling .private() to the end of your Syft Tensor, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765d97-e091-4351-a0d4-420fb1121c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "# select all of Country 0's data\n",
    "country0_data = raw_data.T[0]\n",
    "\n",
    "# Specify it to be a np.int32 dtype\n",
    "country0_data = country0_data.astype(np.int32)\n",
    "\n",
    "# Create an Entity or Data Subject corresponding to country1\n",
    "from syft.core.adp.entity import Entity\n",
    "country0 = Entity(name=\"Country 0\")\n",
    "\n",
    "# Create a Syft tensor\n",
    "data = sy.Tensor(country0_data)\n",
    "\n",
    "# Add DP metadata\n",
    "data = data.private(min_val=0, max_val=150000, entities=country0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66daad-d51b-43cf-bf31-b8687aa31cb9",
   "metadata": {},
   "source": [
    "Now that we're familiar with how to do this, let's repeat this process for all the Countries in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f741fe4-5adf-4406-8727-18af72477dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"Country 0\" : data}\n",
    "entities = []\n",
    "for i in range(1, raw_data.shape[-1]):\n",
    "    country_name = f\"Country {i}\"\n",
    "    \n",
    "    # Create a new Entity correspoinding to the country and add it to the list\n",
    "    new_entity = Entity(name=country_name)\n",
    "    entities.append(new_entity)\n",
    "    \n",
    "    # Add it to the Dataset Dictionary\n",
    "    entry = sy.Tensor(raw_data[:, i].astype(np.int32)).private(min_val=0, max_val=150000, \n",
    "                                                               entities=new_entity)\n",
    "    dataset[country_name] = entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e913e3-0831-42bf-8017-0d94de7c9ece",
   "metadata": {},
   "source": [
    "### 3.5.3. Loading the data to PyGrid!\n",
    "\n",
    "Once the appropriate data, metadata, and labels are prepared, uploading the data to your domain node is as simple as running a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7642b-00f2-486a-80d8-9fee4a41bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Domain Node\n",
    "domain_node = sy.login(email=\"info@openmined.org\", password=\"changethis\", port=8081)\n",
    "\n",
    "# Upload the dataset!\n",
    "domain_node.load_dataset(\n",
    "    assets=dataset, \n",
    "    name=\"COVID19 Cases in 175 countries\", \n",
    "    description=\"Weekly data for an entire year\", \n",
    "    metadata=\"Any metadata you'd like to include goes here\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6b255-8490-412f-9a87-e6fefd6d56d8",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* PyGrid will raise a warning if the data you've uploaded isn't compatible with the Differential Privacy framework we've made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397bf1c-c32e-4dbd-a53d-d5464b37c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_node.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075926db-d98b-47a7-918d-e5e63e4c17d5",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0cc7fd-e3a4-4939-97b7-54576f80bc75",
   "metadata": {},
   "source": [
    "## 3.6. Linking Data From Multiple Sources\n",
    "\n",
    "In this course, we'll be using the PySyft and PyGrid frameworks to link data from multiple sources (called nodes). This is both really cool and very useful because it lets us perform data science and machine learning on private data on someone else's machine or server, without compromising the privacy of anyone in the dataset.\n",
    "\n",
    "\n",
    "In remote data science, because there's a high likelihood that all of our data is not coming from the same source, proper data annotation and cleaning becomes particularly important. This is an important distinction, because:\n",
    "* Different sources may generate data at different rates; some sources stream data whereas others produce data in batches (i.e. in a periodic manner, or at a certain time interval)\n",
    "* Different sources may also have different measuring capabilities, and this might affect the reliability of a dataset. \n",
    "* Different nodes may have different privacy budgets alloted to their respective datasets, which means some datasets may be seen and used much less than others.\n",
    "\n",
    "<b> DID YOU KNOW? </b> A historical example of data sources having different measuring capabilities were the Geiger counters used in Chernobyl. <p>Immediately after the Chernobyl nuclear accident, many people at the time weren't too concerned because measurements from their Geiger counters showed a measurement of 3.6 Roentgen/hour- the equivalent measurement of 10 X-rays. However, it was later discovered that the Geiger counters in use had a maximum detection limit and sensitivity which meant they couldn't detect or measure radiation amounts higher than 3.6 Roentgen per hour. \n",
    "    \n",
    "When new, higher range Geiger counters were used, it was quickly (and shockingly) realized that the radiation being leaked wasn't 3.6 R/h, but around 5.6 R per SECOND. This was the equivalent of 2 Hiroshima nuclear bombs worth of radiation being released per hour.</p>\n",
    "\n",
    "Although this is an extreme example, it shows the importance of proper data acquisition and annotating data. Had the authorities known the differences of their data sources at the time, they might have intervened and helped out sooner, and could have saved countless more lives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989f9aa-9d92-4acd-b408-cec78d3353bb",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9712504-7093-4645-b20b-b94185d4e1e0",
   "metadata": {},
   "source": [
    "## Additional Resources and References\n",
    "\n",
    "<ul><li> Lyko K., Nitzschke M., Ngonga Ngomo AC. (2016) Big Data Acquisition. In: Cavanillas J., Curry E., Wahlster W. (eds) New Horizons for a Data-Driven Economy. Springer, Cham. <a> https://doi.org/10.1007/978-3-319-21569-3_4 </a> </li>\n",
    "    <li> <a href=\"https://www.oreilly.com/library/view/implementing-a-smart/9781491983492/\"> Implementing a Smart Data Platform, O'Reilly Media 2017 </a> </li>\n",
    "    <li> <a href=\"https://www.oreilly.com/library/view/python-data-cleaning/9781800565661/\"> Python Data Cleaning, O'Reilly Media 2020 </a></li>\n",
    "    <li> <a href=\"https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html\"> Examples of Image Annotation Types </a></li>\n",
    "    <li> <a href=\"https://developers.google.com/machine-learning/glossary#label\"> Good description of annotations / labels </a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4500071-50fb-494c-b2d0-8e9ba6a18877",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693b37c-6e72-46d3-9f84-e0e1c565a20c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
